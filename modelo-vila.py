from transformers import AutoProcessor, AutoModelForVision2Seq
import torch
from PIL import Image

# Configuraci√≥n de dispositivo
device = "cuda" if torch.cuda.is_available() else "cpu"

# Cargar el modelo y el processor desde Hugging Face
processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")
model = AutoModelForVision2Seq.from_pretrained("llava-hf/llava-1.5-7b-hf").to(device)

# Prompt complejo
prompt = (
    "Ahora te enviar√© una imagen de un auto.\n\n"
    "Necesito que la analices y me devuelvas la siguiente informaci√≥n en este formato exacto:\n"
    "Color: <color principal del auto>\n"
    "Matr√≠cula: <matr√≠cula del auto, solo letras y n√∫meros, sin guiones ni espacios>\n"
    "Marca: <marca del auto, por ejemplo: Toyota, Ford>\n"
    "Modelo: <modelo del auto, por ejemplo: Corolla, Fiesta>\n\n"
    "‚ö†Ô∏è Importante:\n"
    "- Respond√© usando exactamente cuatro l√≠neas, una para cada √≠tem solicitado, en el orden y formato indicados.\n"
    "- No incluyas texto adicional ni explicaciones.\n"
    "- Si alg√∫n elemento no se puede determinar con claridad, escrib√≠: `no detectado` luego de los dos puntos."
)

# Ruta de la imagen
ruta_imagen = "img.jpg"  # Cambi√° esto por la ruta real de tu imagen

# Cargar y convertir imagen
imagen = Image.open(ruta_imagen).convert("RGB")

# Procesar una sola imagen
inputs = processor(images=imagen, text=prompt, return_tensors="pt").to(device)

# Generar respuesta
outputs = model.generate(**inputs, max_new_tokens=200)
respuesta = processor.batch_decode(outputs, skip_special_tokens=True)[0]

# Mostrar la respuesta
print("\nüßæ Descripci√≥n del auto:")
print(respuesta)
